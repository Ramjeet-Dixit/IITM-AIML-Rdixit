# -*- coding: utf-8 -*-
"""Copy of Data Preprocessing IITM -DRC Week

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TOg_bPO6SN2k5MeFaoyentPfPCwgEo9s
"""

# End-to-End Preprocessing with Scikit-learn and imbalanced-learN

# ‚úÖ Install Required Libraries
!pip install -q imbalanced-learn

# ‚úÖ Import Required Libraries
import numpy as np
import pandas as pd
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, PowerTransformer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

# ‚úÖ Load Sample Data: Adult Income Dataset from OpenML
# This dataset contains demographic information and income classification
# We will predict whether income exceeds $50K/yr based on census data
data = fetch_openml("adult", version=2, as_frame=True)
df = data.frame

# Drop missing values for simplicity
df = df.dropna()

# Separate features and target
X = df.drop("class", axis=1)
y = df["class"]

# ‚úÖ Identify Categorical and Numerical Columns
cat_cols = X.select_dtypes(include="category").columns.tolist()
num_cols = X.select_dtypes(include=["float64", "int64"]).columns.tolist()

# ‚úÖ Train-Test Split
# We use stratified splitting to maintain class distribution
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# ‚úÖ Preprocessing Pipelines

# Pipeline for Categorical Features:
# 1. Impute missing values using the most frequent value
# 2. One-hot encode the categories
cat_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="most_frequent")),
    ("encode", OneHotEncoder(handle_unknown="ignore"))
])



# Pipeline for Numerical Features:
# 1. Impute missing values using KNN
# 2. Standard scaling
num_pipeline = Pipeline([
    ("impute", KNNImputer(n_neighbors=3)),
    ("scale", StandardScaler())
])

# Combine both pipelines using ColumnTransformer
preprocessor = ColumnTransformer([
    ("cat", cat_pipeline, cat_cols),
    ("num", num_pipeline, num_cols)
])

# ‚úÖ Full Pipeline with SMOTE Balancing and Logistic Regression
# Using imbalanced-learn's Pipeline to integrate preprocessing + SMOTE + model
model_pipeline = ImbPipeline(steps=[
    ("preprocess", preprocessor),
    ("balance", SMOTE(random_state=42)),
    ("clf", LogisticRegression(max_iter=1000))
])

# ‚úÖ Train the Model
model_pipeline.fit(X_train, y_train)

# # üìä Exploratory Data Analysis (EDA)

## üí° Dataset Overview
print("Shape of dataset:", df.shape)
df.head()

## üßÆ Basic Statistics
print(df.describe(include='all'))

## üìå Missing Values
print("Missing values per column:")
print(df.isnull().sum())

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

## üè∑Ô∏è Class Distribution
sns.countplot(data=df, x="class")
plt.title("Class Distribution")
plt.xticks(rotation=45)
plt.show()

## üîç Categorical Feature Distributions
cat_cols = X.select_dtypes(include="category").columns.tolist()
for col in cat_cols:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=col, hue="class")
    plt.title(f"Distribution of {col} by Class")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

## üìà Numerical Feature Distributions
num_cols = X.select_dtypes(include=["float64", "int64"]).columns.tolist()
df[num_cols].hist(figsize=(12, 10), bins=20)
plt.suptitle("Histograms of Numerical Features")
plt.show()

## üìâ Boxplots for Outlier Detection
for col in num_cols:
    plt.figure(figsize=(6, 4))
    sns.boxplot(data=df, x=col)
    plt.title(f"Boxplot of {col}")
    plt.tight_layout()
    plt.show()

## üìä Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

# ‚úÖ Evaluate the Model
# Print classification report on test data
y_pred = model_pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

# üìå Summary of Preprocessing Steps Applied:
# 1. Handling missing data (SimpleImputer, KNNImputer)
# 2. Encoding (OneHotEncoder)
# 3. Scaling (StandardScaler)
# 4. Balancing (SMOTE)
# 5. Integration using Pipeline and ColumnTransformer
# 6. Model: Logistic Regression
# 7. Evaluation: Classification Report

